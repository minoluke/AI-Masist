[
    {
        "Name": "signaling_norms_emergence_llm",
        "Title": "The Spontaneous Emergence of Signaling Norms in LLM-Agent Societies: How Costly Signals Facilitate Parochial Cooperation",
        "Background": "Social norms often involve seemingly arbitrary or costly behaviors (e.g., specific greetings, dress codes) that signal group membership and facilitate trust and cooperation within groups (parochial cooperation). Theoretical and agent-based models in sociology and biology have shown that such signaling norms can emerge spontaneously, particularly in minority groups or under intergroup conflict. Recent advances in LLM-based multi-agent systems offer a novel testbed to explore the emergence of these norms in a more realistic, language-rich setting where agents can interpret, assign meaning to, and innovate signals through natural language interaction. While existing LLM-agent research focuses on norm compliance, learning, and enforcement (e.g., GovSim, CRSEC, Normative Modules), the endogenous emergence of signals *as* norms\u2014where arbitrary behaviors acquire signaling value through social learning and evolutionary pressure\u2014remains underexplored.",
        "Objective": "To investigate whether and how LLM agents, interacting in a mixed-motive social dilemma with group distinctions, can spontaneously develop costly signaling norms that facilitate within-group trust and cooperation (parochial cooperation). We aim to observe the process of signal meaning creation, adoption, and stabilization, and to test key predictions from signaling theory (e.g., costliness ensures signal honesty).",
        "Research Question": "1. Under what conditions (e.g., group size asymmetry, intergroup competition, signal cost) do LLM agents develop distinct, costly signaling norms to identify in-group members? 2. Do such emergent signaling norms effectively increase trust and cooperation within groups compared to between groups? 3. How does the semantic content and perceived 'meaning' of the signal evolve through agent communication and reasoning?",
        "Hypothesis": "1. Costly (but initially meaningless) signals are more likely to emerge and stabilize as markers of group identity in smaller or disadvantaged groups where the benefits of within-group cooperation are highest. 2. Once established, agents using the same costly signal will show higher levels of trust and cooperation in dyadic interactions (trust games) compared to agents with different or no signals. 3. Signals with moderate cost will be most effective\u2014high enough to deter free-riders but low enough to be adopted. 4. Agents will develop shared natural language justifications/rationales for the signal, giving it 'meaning' post-hoc.",
        "Related Work": "Related work falls into three categories: (1) **Classical signaling & norm models** (e.g., Macanovic et al., 2024 - 'Signals of belonging') use traditional ABM to show costly signals emerge in minority groups. Our work replicates and extends this with LLM agents capable of rich interpretation. (2) **LLM-agent norm emergence** (e.g., Ren et al., 2024 - CRSEC; Sarkar et al., 2024 - Normative Modules) focuses on architecting norm representation, spreading, and compliance. We differ by studying norms that originate as arbitrary signals rather than prescribed rules. (3) **LLM-agent cooperation studies** (e.g., Piatti et al., 2024 - GovSim; Gupta et al., 2025) examine cooperation in resource dilemmas with explicit reward structures. We focus on the trust game with endogenous signal formation. Our key distinction is the **semiotic process**: how LLMs, through interaction, transform arbitrary actions into meaningful social signals.",
        "Abstract": "This study proposes a multi-agent simulation to investigate the spontaneous emergence of signaling norms in societies of LLM-based agents. We design a mixed-group environment where agents from two populations (Group A and B) repeatedly engage in dyadic trust games. Crucially, agents can adopt and display a 'signal'\u2014an initially meaningless tag or phrase that carries a small cognitive or social cost to use. Agents decide whether to adopt a signal, which signal to adopt, and whether to trust/interact with others based partially on observed signals. Through social learning (observing successful interactions) and reinforcement, we predict that distinct, costly signaling norms will emerge within each group, particularly the minority group, as a means to identify cooperative in-group members. We will track the adoption dynamics, the evolution of trust and cooperation rates within vs. between groups, and the natural language justifications agents generate for their signal use. This work bridges signaling theory from evolutionary social science with the rich semantic capabilities of LLM agents, offering new insights into the origins of social symbols and parochialism.",
        "Agents": {
            "Count": "40-100 agents (configurable, e.g., 60 agents: 40 in Group A, 20 in Group B to create size asymmetry)",
            "Roles": "Two group affiliations (Group A, Group B). All agents are functionally identical\u2014they can be Trustors or Trustees in the trust game. No pre-defined personalities; initial attitudes are neutral.",
            "State Specification": "Each agent has: (1) `group_id` (A or B), (2) `signal` (one of S1, S2, S3, or NULL), (3) `memory` of past interactions (list of tuples: partner_id, partner_signal, role, amount_sent, amount_returned, outcome_utility), (4) `trust_threshold` (a dynamic parameter influencing willingness to trust based on partner's signal alignment), (5) `internal_justification` (a text field storing the agent's reason for its current signal choice, updated via LLM reflection).",
            "State Update": "After each interaction, agents update their memory. Periodically (e.g., every 5 rounds), agents undergo a 'reflection' phase using an LLM call to update their `trust_threshold` and `internal_justification` based on their interaction history. Signal updates occur via an imitation process: agents may copy the signal of a successful agent (high cumulative payoff) from their own group with some probability.",
            "Environment Interaction": "Agents interact in pairwise trust games within a structured interaction protocol. They observe the potential partner's `group_id` and `signal` before deciding to interact."
        },
        "Environment": {
            "Structure": "Agents are placed in two distinct groups (A and B). Interaction network: each round, agents are randomly paired within a large 'mixing pool' that allows both within-group and between-group pairing. No spatial grid; focus is on group identity and signals.",
            "State Specification": "Environment state includes: (1) current round, (2) prevalence of each signal within each group, (3) average cooperation rates per signal pair. This global information is not visible to agents.",
            "Update Rules": "The environment manages the pairing, conducts the trust game, and calculates payoffs. It does not change endogenously except for tracking aggregate statistics."
        },
        "Protocol": {
            "Turn Structure": "Synchronous discrete rounds. Each round: 1. Pairing phase. 2. Interaction phase (all pairs play trust game simultaneously). 3. Learning/update phase (asynchronously, agents update internal state based on outcome). 4. (Every N rounds) Signal imitation phase.",
            "Termination Condition": "After a fixed number of rounds (e.g., 200) OR when signal adoption within each group reaches a stable equilibrium (e.g., >80% of a group shares one signal for 20 consecutive rounds).",
            "Number of Trials": "At least 30 independent simulation runs per experimental condition to ensure statistical robustness.",
            "Interaction Flow": "For each paired agent (i, j): 1. Both agents observe each other's group_id and signal. 2. Agent i (randomly assigned as Trustor) decides whether to 'pass' (not interact, both get 0) or to send an endowment of X points. Decision uses a stochastic rule based on `trust_threshold` and signal match. 3. If i sends X points, the amount is multiplied (e.g., by 3) and given to j (Trustee). 4. Trustee j decides what fraction (0 to 1) of the multiplied amount to return to i. 5. Payoffs are calculated. 6. Both agents record the interaction in memory."
        },
        "Rules": {
            "Public Information": "The rules of the trust game (endowment amount, multiplication factor), the set of possible signals (e.g., three colors: 'Blue', 'Red', 'Green'), and the fact that signals may have a cost are known to all.",
            "Private Information": "Each agent's internal `trust_threshold`, `memory`, and `internal_justification` are private. An agent's group affiliation is observable.",
            "Decision Rules": "Trustor's send decision: Probability(send) = sigmoid(\u03b2 * (\u03b8 + \u03b1 * I(signal_match) - \u03b3 * I(different_group))). Where \u03b8 is trust_threshold, \u03b1 is signal bonus weight, \u03b3 is group penalty weight, \u03b2 is temperature. I() is indicator. Trustee's return fraction: Determined by an LLM call given context (self's role, partner's group & signal, amount received, recent history). LLM is prompted to output a number between 0 and 1.",
            "Payoff Structure": "If no interaction: both get 0. If interaction: Trustor payoff = (Endowment - Sent) + Returned. Trustee payoff = (Multiplier * Sent) - Returned - Signal_Cost. Signal_Cost is a small fixed point deduction applied in every round the agent uses a non-NULL signal (cost ensures signal is 'costly')."
        },
        "Experimental Conditions": {
            "Independent Variables": [
                "Group size ratio (Majority-Minority: e.g., 50-10, 30-30)",
                "Signal cost (None, Low, Medium, High)",
                "Availability of signals (2 vs. 3 possible signals)",
                "Learning rule (pure imitation vs. payoff-biased imitation vs. LLM-reflective choice)"
            ]
        },
        "Baseline": "Condition with Zero signal cost and no group distinction (or a single group). This baseline tests if signals emerge without cost/group pressure.",
        "Parameter Sweep": "Signal cost: [0, 0.5, 1, 2] points per round. Group size ratio: Majority:Minority from [1:1] to [4:1]. Payoff multiplier in trust game: [2, 3, 4].",
        "Logging Content": "Per round, per agent: agent_id, group, current_signal, trust_threshold, payoff. Per interaction: pair_ids, signals, groups, sent_amount, returned_amount, interaction_type (within/between group). Per imitation event: copying agent, source agent, signal copied. Periodic snapshots of agents' `internal_justification` texts.",
        "Log Format": "CSV files for quantitative data (agents, interactions). JSON lines for qualitative `internal_justification` texts with timestamps.",
        "Analysis Metrics": [
            "Signal Adoption Dynamics: Proportion of each signal per group over time.",
            "Within-Group vs. Between-Group Cooperation Rates: Comparison of send rates and return fractions.",
            "Efficiency: Average total payoff per capita in majority vs. minority groups.",
            "Signal Honesty Correlation: Correlation between signal use and cooperative behavior (return fraction) across agents.",
            "Semantic Analysis of Justifications: Using NLP (topic modeling, sentiment) to track how agents rationalize their signal use."
        ],
        "Verification Method": "Statistical tests (t-tests, ANOVA) to compare cooperation rates between conditions. Time-series analysis (regression) for adoption dynamics. Qualitative coding of justification texts to identify themes (e.g., 'signal shows trustworthiness', 'signal is our tradition').",
        "Risk Factors and Limitations": "(1) **LLM Instability & Cost**: LLM calls for trustee decisions and reflections are expensive and may introduce non-determinism. Mitigation: Use smaller open-source LLMs for core decisions, cache responses, and set strict random seeds. (2) **Interpretability**: The 'black box' nature of LLM decision-making may complicate causal inference. We will use structured decision rules for trustors to maintain control. (3) **Scalability**: Running 100 agents for 200 rounds with LLM calls is computationally intensive. We will use batched calls and limit the frequency of reflection phases. (4) **Generalizability**: Findings may be specific to the chosen LLM's propensity for social reasoning. We will test with at least two different LLM backbones (e.g., GPT-4o and Llama 3)."
    },
    {
        "Name": "value_negotiation_hybrid_norms",
        "Title": "Negotiating Values, Emergent Hybrid Norms: LLM Agents in Culturally Diverse Resource Dilemmas",
        "Background": "In an increasingly globalized world, interactions between individuals with different cultural values (e.g., individualism vs. collectivism) are common, leading to potential conflicts in cooperative settings like shared resource management. Traditional agent-based models of cultural diffusion (e.g., Axelrod's model) and recent LLM-agent studies on norm formation focus on alignment, compliance, or the spread of existing norms. However, how agents with deeply held, divergent cultural values negotiate and spontaneously create new, hybrid norms through dialogue and social learning remains underexplored. LLM agents, with their rich natural language capabilities, offer a unique testbed to simulate such value-laden negotiations and observe the emergent normative order.",
        "Objective": "To investigate how LLM agents with different cultural value orientations negotiate and develop hybrid cooperation norms in a common-pool resource dilemma. We aim to observe the process of value conflict, dialogue-based negotiation, and the emergence of stable, shared norms that blend elements from different cultures.",
        "Research Question": "1. How do LLM agents with contrasting cultural values (individualistic vs. collectivist) initially behave in a common-pool resource game, and how does this lead to conflict? 2. Through dialogue and social learning, do agents converge on a hybrid norm for resource use, and what characteristics does this norm have? 3. How does the presence of communication channels (open dialogue vs. no communication) affect the speed and stability of norm emergence and resource sustainability?",
        "Hypothesis": "1. Without communication, agents will act according to their cultural values (individualists over-harvest, collectivists restrain), leading to resource depletion and low collective welfare. 2. With open dialogue, agents will engage in value negotiation, leading to the emergence of a hybrid norm that balances individual and collective interests (e.g., moderate harvesting rules). 3. The hybrid norm will be more sustainable and efficient than pure-value-based behaviors, and agents will develop shared justifications that reference both cultural perspectives. 4. Social learning (imitating successful agents) will accelerate norm convergence, especially when combined with dialogue.",
        "Related Work": "Related work includes: (1) **Cultural diffusion models** (Axelrod, 1997) simulate how cultural traits spread but assume fixed traits and no negotiation. (2) **LLM-agent norm formation** (e.g., Gupta et al., 2025; Ren et al., 2024) focuses on norm compliance and learning in homogeneous groups. (3) **Cultural alignment in LLMs** (e.g., Yuan et al., 2024 - Cultural Palette) aims to align LLMs with specific cultural values, but not on inter-cultural negotiation. (4) **Value-aware agents** (e.g., Mohamadi & Yavari, 2025 - DECIDE-SIM) study ethical choices but not cultural value negotiation. Our work uniquely combines **cultural value diversity**, **resource dilemma**, and **LLM-mediated dialogue** to study hybrid norm emergence, going beyond alignment or diffusion.",
        "Abstract": "This study proposes a multi-agent simulation where LLM agents with different cultural value orientations (individualistic vs. collectivist) interact in a common-pool resource dilemma. Agents must harvest resources from a shared pool to earn points, but over-harvesting leads to resource collapse. Crucially, agents can engage in natural language dialogue before making harvesting decisions, allowing them to negotiate acceptable behaviors based on their values. We vary communication conditions (no communication, open dialogue) and social learning mechanisms (imitation of successful agents). We track harvesting actions, dialogue content, and the evolution of personal norms. We hypothesize that dialogue enables value negotiation, leading to emergent hybrid norms that sustainably manage resources. This work contributes to understanding how diverse societies can co-create cooperative institutions through communication, with implications for AI-mediated intercultural collaboration.",
        "Agents": {
            "Count": "30-60 agents (configurable, e.g., 30: 15 individualists, 15 collectivists)",
            "Roles": "Two cultural value orientations: Individualist (prioritizes personal gain) and Collectivist (prioritizes group welfare). All agents are resource harvesters; no other roles.",
            "State Specification": "Each agent has: (1) `cultural_value` (individualist or collectivist), (2) `personal_norm` (a target harvesting amount, initially set based on culture), (3) `memory` of past interactions (own harvest, others' harvests, dialogue history, resource state, personal payoff), (4) `trust_level` (dynamic, affects willingness to follow negotiated norms), (5) `dialogue_history` (recent conversations for context).",
            "State Update": "After each round, agents update memory. Periodically (every 5 rounds), agents reflect via LLM call to update `personal_norm` and `trust_level` based on experience and dialogue. Social learning: agents may imitate the `personal_norm` of a high-payoff neighbor with probability.",
            "Environment Interaction": "Agents interact with the shared resource pool by choosing a harvest amount. They can send and receive dialogue messages to a subset of agents (e.g., within a communication network) before harvesting."
        },
        "Environment": {
            "Structure": "Shared resource pool with renewable dynamics (e.g., logistic growth). Agents are placed on a small-world network that defines both resource access neighborhoods and communication links.",
            "State Specification": "Resource pool: `current_resource_level`, `carrying_capacity`, `growth_rate`. Network structure: adjacency matrix for communication.",
            "Update Rules": "Resource update: after all agents harvest, resource grows: `resource_{t+1} = resource_t + growth_rate * resource_t * (1 - resource_t / carrying_capacity) - total_harvest`. If resource falls below a threshold, it collapses (zero growth)."
        },
        "Protocol": {
            "Turn Structure": "Synchronous discrete rounds. Each round: 1. Dialogue phase (agents exchange messages with neighbors via LLM calls). 2. Harvest decision phase (each agent chooses harvest amount based on personal_norm, dialogue, and trust). 3. Resource update and payoff calculation. 4. Learning phase (social imitation and reflection, asynchronous).",
            "Termination Condition": "After a fixed number of rounds (e.g., 100) OR if resource collapses for 5 consecutive rounds.",
            "Number of Trials": "At least 20 independent runs per experimental condition for statistical significance.",
            "Interaction Flow": "For each round: (1) Dialogue: each agent generates a message to each neighbor (LLM call with context: cultural value, recent history, resource state). Messages are shared. (2) Harvest: each agent decides harvest amount (integer 0 to max_allowed) via a decision function that considers personal_norm, messages received, and trust. (3) Environment updates resource and calculates payoffs (payoff = harvest_amount * constant - penalty if over-harvesting). (4) Agents update memory and possibly adjust personal_norm."
        },
        "Rules": {
            "Public Information": "Basic rules of the resource pool (growth rate, carrying capacity), the fact that over-harvesting can lead to collapse, and the communication protocol are known.",
            "Private Information": "Each agent's `cultural_value`, `personal_norm`, `trust_level`, and `memory` are private. Dialogue messages are private to the sender and recipient.",
            "Decision Rules": "Harvest decision: `harvest_amount = clip(personal_norm + influence_from_dialogue + noise, 0, max_harvest)`. Influence_from_dialogue is determined by an LLM call that interprets received messages and outputs an adjustment. Dialogue generation: LLM call with prompt to express values, suggest norms, or respond to others.",
            "Payoff Structure": "Payoff = `harvest_amount * point_per_unit` - `overharvest_penalty` if `total_harvest > sustainable_threshold`. Sustainable_threshold is a fraction of current resource level."
        },
        "Experimental Conditions": {
            "Independent Variables": [
                "Communication condition (no communication, open dialogue)",
                "Social learning (on/off)",
                "Cultural composition (ratio of individualists to collectivists: 50-50, 70-30)",
                "Resource scarcity (high vs. low growth rate)"
            ]
        },
        "Baseline": "Condition with no communication and no social learning, where agents act solely based on initial cultural values.",
        "Parameter Sweep": "Growth rate: [0.1, 0.3, 0.5]; carrying capacity: [100, 200]; overharvest_penalty: [0, 5, 10]; noise in harvest decision: [0, 1, 2].",
        "Logging Content": "Per round: resource level, total harvest, average harvest per cultural group. Per agent: harvest amount, personal_norm, payoff, cultural value. Per dialogue: sender, receiver, message content. Per learning event: imitation actions.",
        "Log Format": "CSV for quantitative data (resource, harvests, payoffs). JSON lines for dialogue messages with metadata.",
        "Analysis Metrics": [
            "Resource sustainability (time to collapse or final resource level)",
            "Norm convergence (variance of personal_norms over time, within and between groups)",
            "Efficiency (total payoff per capita)",
            "Dialogue analysis: frequency of value-related terms (e.g., 'self', 'group'), sentiment, negotiation patterns",
            "Equity (Gini coefficient of payoffs across agents)"
        ],
        "Verification Method": "Statistical tests (ANOVA) to compare sustainability and efficiency across conditions. Time-series analysis for norm convergence. NLP techniques (keyword extraction, topic modeling) on dialogue corpus to identify value negotiation themes. Correlation between dialogue content and subsequent harvest decisions.",
        "Risk Factors and Limitations": "(1) **LLM cost and variability**: Frequent LLM calls for dialogue and decisions can be expensive and non-deterministic. Mitigation: Use smaller LLMs for dialogue generation, cache responses, and set random seeds. (2) **Simplified cultural model**: Reducing culture to binary values may overlook nuances. We will incorporate Hofstede's dimensions as continuous variables in extended studies. (3) **Scalability**: Large agent counts with dialogue may be computationally heavy. We limit agents to 60 and use batched LLM calls. (4) **Generalizability**: Findings may depend on the LLM's built-in cultural biases. We will test with multiple LLM backbones (e.g., GPT-4, Llama 3) to assess robustness."
    }
]