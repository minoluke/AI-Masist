"""
実験タスクとメトリクスの定義
新しい実験を追加する場合はここに追記してください
"""

# =============================================================================
# TPGG (Threshold Public Goods Game)
# =============================================================================
TPGG_TASK_DESC = """
シミュレーション検討シート（TPGG）
1. シミュレーション要求
背景・文脈：
4人グループで「一定以上のお金（トークン）をみんなで出し合えたらご褒美がもらえる」というゲームがあります。
 ただし、ご褒美がもらえるライン（＝しきい値）にギリギリ必要な額を目指すのか、
 それより多めの額を要求するのかによって、みんなの出し方が変わると言われています。
この「必要な分ちょうど」と「必要以上に多め」の違いが、
 実際の行動にどう影響するのかを LLM で調べます。

目的：
グループに「出してほしい金額の目安（ルール）」を示したとき、
 そのルールが「ちょうど必要な合計」か「必要以上に多い合計」かによって、
 行動や結果がどう変わるのかを調べる。



研究質問：
ルールが「必要以上に多い（多めの要求）」だと、行動が乱れやすい？


必要な分ちょうどのルールは、むしろ安定した協力を生む？


グループ全体がちょうど必要額に合わせる「効率の良さ」はどう変わる？



仮説：
① 多めに要求されたルールは、守る人が減りやすい。


② 必要額を達成できるかどうかは、ルールの違いではあまり変わらない。


③ 多めのルールは「出しすぎ（無駄）」を増やし、効率を下げる。


④ 1人あたりの負担がピッタリ均等割りできる場合、協力がまとまりやすい。



2. シミュレーション要件

エージェント
人数：
 4人（1グループ）
ロールと説明：
4人とも同じ立場。


各ラウンドで、自分の10トークンのうち何トークンを共同の箱に入れるか選ぶ。


全員の合計がしきい値を超えれば、ご褒美 (V) がもらえる。


記憶・内部状態：
過去の自分の拠出額


グループ合計の拠出額


自分の得点
 -（後半だけ）自分に示された「出してほしい額（ルール）」


更新ルール：ラウンドの最後に、そのラウンドの情報を記録して次のラウンドの参考にする。



プロトコル
ターン/ラウンド構造：
1回のゲームは20ラウンド。


各ラウンドは


4人が同時に出す額を決める


合計額を見る


しきい値を超えたかどうか判定


得点を返す


最大ラウンド数：
 20
終了条件：
 20ラウンド終わったら終了。
各工程の試行数：
1つの設定につき、4人グループを複数回（例：11グループ）まわす。


フェーズ構造（任意）：
ラウンド1〜10：ルールなし


ラウンド11〜20：設定に応じたルールを提示（またはなし）



環境・ルール

ネットワーク構造：
4人は同じグループ


グループ間の交流はなし
 （＝4人だけの閉じた小世界で毎回意思決定）



行動空間 / アクションセット：
桁数：0〜10 の整数から1つ選んで出すだけ。



共有情報：
各自の持ちトークンは10


しきい値 (T)（条件ごとに違う）
 -（後半）みんなの「出してほしい額（ルール）」


ラウンドが終わった後の


合計拠出額


自分の得点



非公開情報：
他のメンバーが実際にいくら出したかは見えない（自分の分は見える）


各メンバーの考え・意図



必要なルール、利得構造：
利得（1人あたり）：
自分が出した額を (c_i)


全員の合計を (C = \sum c_i)


しきい値未達成（C < T）：
 [
 \pi_i = 10 - c_i
 ]
しきい値達成（C \ge T）：
 [
 \pi_i = 10 - c_i + V
 ]
しきい値を超えた分には追加のご褒美なし
 （＝出しすぎは「無駄」）



実験条件（＝比較する設定の一覧）
5つの設定を作る：
必要額ちょうど・均等割り OK（FAIRSUFF）


T = 20


ルール = (5,5,5,5)


多めの要求・均等割り OK（FAIRINF）


T = 20


ルール = (5,5,6,6)


必要額ちょうど・均等割り不可（UNFAIRSUFF）


T = 22


ルール = (5,5,6,6)


多めの要求・均等割り不可（UNFAIRINF）


T = 22


ルール = (6,6,6,6)


ルールなし（CONTROL）


T = 22


ルールなし



ログ・分析指標
ログ形式：
記録すべき内容：
ラウンド番号


各メンバーの出した額


合計出した額


しきい値達成の有無


各メンバーの得点
 -（後半）ルールを守ったかのフラグ


どの設定で行ったか


分析指標：
しきい値達成率（成功の割合）


平均の出した額


必要額よりどれだけ多く出たか（過剰分）


ルールを守った割合


10ラウンド目→11ラウンド目の変化（ルール導入効果）


"""

TPGG_METRICS = [
    "threshold_achievement_rate",
    "average_contribution",
    "excess_contribution",
    "rule_compliance_rate",
]


# =============================================================================
# BBS (Bulletin Board System) - LLM掲示板影響モデル
# =============================================================================
BBS_TASK_DESC = """
シミュレーション検討シート
テーマ：LLM 掲示板影響モデル

1. シミュレーション要求
背景・文脈
オンライン掲示板は、多人数による投稿・返信を通して意見の同調、分極化、少数意見の持続や断片化などが生じる典型的な社会的場である。
LLM マルチエージェントを用いると、人間実験より低コストかつ大規模にこうしたダイナミクスを再現できることが示されている。掲示板型フォーラムを扱う研究では、複数のLLMをペルソナ付きエージェントとして配置し、人間の議論に見られる現象を再現可能であることが報告されている。

目的
掲示板スレッド環境に複数のLLMエージェントを配置し、人間フォーラムで見られる同調・分極化・断片化がどれほど再現されるかを調べる。
モデル規模および推論特化モデルの特性が、同調率・分極化などの指標に与える影響を比較する。

研究質問
RQ1: LLM同士の掲示板議論は、人間の議論と同様の集団ダイナミクスを示すか。
RQ2: モデル規模や推論特化モデルの有無は、同調率・分極化・断片化にどのような差をもたらすか。
RQ3: 初期意見のばらつきやトピックの性質は、議論のダイナミクスにどれほど影響するか。

仮説
H1: 小型モデルや一部の汎用モデルは同調率が高く、多数派への収束が起こりやすい。
H2: 推論特化モデルは同調率が低く、多様性や断片化を維持しやすい。
H3: すべてのモデル群で、議論の進行とともに意見が極端化しうるが、その強さはモデル群によって異なる。
H4: 初期少数派意見は、推論特化モデルでは長期間維持される一方、同調傾向の強いモデルでは早期に消失しやすい。

その他（任意）
AutoGen 系のマルチエージェントフレームワークを用いて実装する前提。

2. シミュレーション要件

エージェント
人数
基本は 6 エージェント。
必要に応じて 20 エージェント規模に拡張し、少数意見維持やクラスタ数の変化を観測する。

ロールと説明
6つの代表的ペルソナを設定する例：
- 強い賛成
- やや賛成
- 中立
- やや反対
- 強い反対
- 反骨的・懐疑的な立場
それぞれデモグラフィックやコミュニケーションスタイルも付与する。

エージェントの状態仕様
- persona: 固定ペルソナ情報
- current_stance: 現在のスタンス（例：−2〜+2）
- belief_history: スタンス履歴
- message_history_view: 読み取ったスレッドログ
行動：スレッドログを読んで1投稿を生成し、投稿末尾に自己申告スタンス [STANCE: +1] などを付与。

状態更新
ラウンドごとに投稿内容を自己評価し、スタンスを更新する。
更新ルールは、自然言語による「説得力があると感じたら更新してよい」という形で指定。

エージェントと環境の相互作用
環境がスレッドログを提供 → 各エージェントが投稿 → 環境がログを更新 → 次のラウンドへ。

環境
構造
単一スレッドの掲示板環境。
テーマは価値対立を含む公共政策など。

状態仕様
state = { round_index, thread_log, active_agents }

更新ルール
各ラウンドで全エージェントが1投稿
投稿順序は固定またはランダム
規定ラウンド到達または収束判定で終了

プロトコル
ターン/ラウンド構造
1シミュレーション = 5 ラウンド
各ラウンドで 6 エージェントが1回ずつ投稿
全条件について 25 試行

終了条件
ラウンド5到達
あるいは「全員のスタンスが一定幅に収束」と判断された場合（任意）

フェーズ構造（任意）
Round 1: 初期投稿
Round 2–4: 相互説得
Round 5: 最終意見提示

対話フロー
System：テーマ、出力フォーマット、ルール
Persona：各エージェントの個別設定
Manager ↔ Agent の往復で議論を進行

ルール
共有情報
- テーマ
- これまでのすべての投稿
- 「ペルソナを大きく裏切らない範囲で、説得力があれば意見更新してよい」という共通原則

非公開情報
- 個別の心理的バイアス（協調優先・一貫性優先・対立姿勢など）
- スタンス更新のしやすさ（柔軟／頑固など）

意思決定ルール
論理的一貫性や説得力を評価してスタンスを調整
モデル群ごとに「同調しやすい」「論理重視で揺れにくい」などのプロンプト差をつける

利得構造（任意）
明確な数値利得は設定しない
一貫性・協調性・説得力をメタ的に重視するよう自然言語で指示
必要に応じてメタ評価器が投稿にスコアを付与

実験条件
モデルグループ例
- Group A: 小型モデル
- Group B: 中〜大型モデル
- Group C: フロンティア/商用モデル
- Group D: 推論特化モデル

トピック
- 環境政策
- 移民政策
- プライバシー/セキュリティ
- その他多数の価値対立テーマ

初期スタンス分布
- 対称的分布
- 極端少数派を混ぜた分布
- 中立多め など複数パターン

ログ
形式
記録例:
run_id, model_group, topic_id, round, agent_id, persona, stance_numeric, text, timestamp

記録すべき内容
- 投稿全文
- スタンス（数値・ラベル）
- スタンス分布（平均・分散など）
- 実験条件（モデル種別・トピック・シードなど）

分析指標
主な指標
- 同調率 (Conformity Rate): 多数派スタンスへ近づく方向に変化した割合。
- 分極化 (Polarization): スタンスの分散・絶対値平均の増加量。
- 断片化 (Fragmentation): クラスタ数・クラスタ間距離・マイナークラスタのサイズから評価。

追加指標
- コンセンサス率
- 意見変化総量

仮説検証への利用方法
H1: 同調率をモデル群間で比較
H2: 断片化指標を比較し、多様性維持傾向を見る
H3: 分極化指標の増加度を比較
H4: 少数派の維持時間、反転回数、維持率を比較
"""

BBS_METRICS = [
    "conformity_rate",           # 同調率
    "polarization",              # 分極化
    "fragmentation",             # 断片化
    "consensus_rate",            # コンセンサス率
    "opinion_change_total",      # 意見変化総量
    "minority_survival_rate",    # 少数派維持率
]


# =============================================================================
# ABM (Agent-Based Model) - LLM駆動ABM vs 従来ルールベースABM比較
# =============================================================================
ABM_TASK_DESC = """
シミュレーション検討シート
テーマ：LLM駆動ABM vs 従来ルールベースABM比較

1. シミュレーション要求
背景・文脈：
従来のABMでは，エージェントの意思決定は「しきい値ルール」「単純な効用関数」「強化学習方策」など，事前に固定されたルールで与えられていることが多い。その結果，
- 人間らしい多様な意思決定・説明可能な行動が表現しにくい
- テキスト情報や複雑な文脈を直接扱えない
といった制約がある。
近年，LLMをエージェントの意思決定モジュールとして組み込むことで，ABMに人間らしい認知・推論・コミュニケーションを導入する試みが増えている。

目的：
従来のルールベースABMと，LLM駆動ABMの間で，
- マクロなパターン（交通流・感染拡大・技術採用曲線など）
- 行動の多様性・パス依存性
を比較すること。

LLMを意思決定に用いることで，どの条件で現実データ（観測された交通・感染・技術採用パターンなど）への適合度が向上するかを評価すること。

研究質問：
RQ1: LLMベースの意思決定に置き換えたABMは，従来ルールベースABMと比べて，交通・感染・技術採用などの既知のマクロパターンをどの程度再現できるか。
RQ2: どのような環境条件・プロンプト設計・LLM設定（温度，メモリ長など）のとき，従来ABMよりも現実データへのフィットが改善するか。
RQ3: LLMを用いることで，エージェントの異質性（価値観・リスク態度・社会規範など）はどの程度自然に表現・制御できるか。

仮説：
H1: 同じマクロ条件（価格，政策，ネットワーク構造など）の下で，LLM駆動ABMは従来ABMと同様のマクロパターン（例：S字型の技術採用曲線）を再現しつつ，個票レベルではより多様で説明可能な行動を示す。
H2: テキスト情報（ニュース，広告文，政策説明文など）が重要なドメインでは，LLM駆動ABMの方が従来ABMよりも実データへの当てはまり（RMSE等）が良い。
H3: プロンプト内でエージェントの属性・性格を明示することで，異質性をパラメトリックに制御しつつ，従来より少ないパラメータ設定で複雑な行動分布を得られる。

その他（任意）：
初期段階では「技術採用ABM（例：EV普及）」をベースラインとし，将来的に交通・感染モデルへ拡張する統一フレームワークを想定。
実装は後にAutoGen/AG2等のフレームワーク上で自動実験可能な形に整理することを前提。

2. シミュレーション要件
エージェント
人数：
個人エージェント 1,000〜10,000 人（デフォルト 2,000）。
感度分析として人数を増減させる。

ロールと説明：
一般消費者エージェント：
- 新技術（例：EV）を採用するかどうかを意思決定。
- 所得，リスク許容度，社会的影響への感受性，環境意識などの属性を持つ。

（拡張）政策決定者エージェント：
- 補助金や規制の水準をラウンドごとに設定（ここもLLM化可能だが，初期バージョンでは固定ルールでもよい）。

エージェントの状態（記憶・内部状態（メモリー）、行動などの仕様）：
静的属性：所得層，年齢層，リスク嗜好（リスク回避〜リスク志向），社会的タイプ（同調的・自律的など）。

動的状態：
- 技術採用状態（未採用／採用済み／乗り換え検討中など）
- 近隣・友人の採用状況の履歴
- 直近数ターンの支出・満足度

メモリ：
- 過去数ターンの観測・自分の選択・結果を要約したテキスト（LLMへのコンテキスト）。

エージェントの状態更新：
各ターンの初めに，環境から観測（価格，補助金，周囲の採用率など）を取得。

シミュレーションエンジンが，
- エージェントの属性・直近メモリ
- 観測された環境状態
をまとめてプロンプトを構築し，LLMに「今ターン，そのエージェントが取るべき行動（採用/未採用など）」を問い合わせる。

LLMの出力（テキスト）をパースして，行動カテゴリにマッピング。
行動に基づいてエージェントの状態（採用状態，メモリ）を更新。

エージェントと環境との相互作用：
エージェントの採用行動が，
- 市場の採用率
- 技術の普及段階
- （拡張）交通需要や感染率
に影響し，次ターンの環境状態として他エージェントに返る。

環境
環境の構造：
社会ネットワーク（グラフ）：
- ノード＝個人エージェント，エッジ＝社会的つながり。
- Small-world / scale-free グラフなどを選択。

市場レベルの集計状態：
- 全体採用率，価格，補助金水準，メディア露出度など。

環境の状態仕様：
グローバル変数：
- 時刻 t
- 採用率，価格，政策レベル
- （オプション）外生ショック（燃料価格高騰など）

ローカル変数（エージェント視点で提示）：
- 近隣の採用率
- 所属コミュニティの平均属性など。

環境の更新ルール：
各ターン後，
- 全エージェントの行動から採用率を再計算。
- 採用率に応じて価格・補助金を調整するシナリオを定義（政策フィードバック）。

交通・感染モデルに拡張する場合は，
- 移動ルート選択 → 混雑度更新
- 接触パターン → 感染率更新
などのドメイン固有の更新式を持つ。

プロトコル
ターン/ラウンド/タイムステップ構造：
離散時間ステップ t = 1, 2, …, T
各ステップで「観測 → LLM呼び出し → 行動 → 環境更新」の順に処理。

最大反復数：
デフォルト T = 100 ターン（技術採用が飽和するまでの時間を想定）。

終了条件：
いずれかを満たした時点で終了：
- 採用率が 95% 以上で 10 ターン連続で変化が小さい
- あるいは t = T に到達。

各工程の試行数：
1条件あたり
- 「従来ルールABM」「LLMハイブリッドABM」を同じシードでペア実行。

フェーズ構造（任意）：
- フェーズ1：ベースライン（政策なし／初期価格一定）
- フェーズ2：政策介入（補助金導入，規制強化など）
- フェーズ3：ショック（燃料価格急騰など）

対話フロー：
シミュレーションエンジンが，各エージェントごとにプロンプトを生成：
- 「あなたは○○な属性を持つ消費者です…」
- 「現在の状況は…」
- 「今ターン，どの選択をしますか？ A: 採用する, B: まだ様子を見る など」

LLMがテキストで理由＋選択肢を返答。
エンジン側で選択肢をパースして行動コードに変換。
必要なら，エージェント同士の会話（口コミ）フェーズも追加可能。

ルール
共有情報（全員が知っている設定・公表情報）：
- 技術の基本的なメリット・デメリット（価格，維持費，環境負荷など）。
- 政策・補助金のルール。
- 行動の選択肢（採用する／しない／保留 など）。

非公開情報（エージェントごとに違う秘密情報）：
- 個人の真の所得，貯蓄制約。
- リスク嗜好，環境意識の強さ。
- 一部のローカル情報（友人から聞いた口コミなど）。

意思決定ルール（任意）：
従来ABM条件：
- ロジットモデルやしきい値モデルで「採用確率」を計算し，確率的に行動。

LLMハイブリッド条件：
- プロンプト内に「あなたの目的は長期的な満足度を最大化すること」「収入制約を超える支出は避けること」などを明示し，
- LLMに推論させて行動を選ばせる。
- 出力は {ADOPT, WAIT, NEVER} などの有限集合にマッピング。

利得構造（任意）：
1ターンごとの効用
u_t = 便益（運転コスト削減・満足度など） − 費用（購入コストの割引分）

長期効用
割引和 ∑_t γ^t u_t を最大化するような行動が望ましい，とLLMプロンプト内に記述。

実験条件：
- ベースライン：従来ルールABMのみ。
- ハイブリッド：エージェントの意思決定をすべてLLMに置き換え。
- ミックス：一部の属性クラス（例：高所得層）のみLLM駆動，他は従来ルール。

LLM内部パラメータ：
- 温度（0.2 / 0.7）
- メモリ長（直近 3 ターン vs 10 ターン）
- プロンプトの詳細度（簡潔 vs 詳細）

ログ
記録すべき内容：
- 時刻 t, エージェントID i
- 状態前後（採用状態，属性の要約）
- LLMプロンプトIDとその要約（完全なテキストは容量の都合で別ストレージに保存）
- LLM応答（選択肢と理由テキスト）
- 実際に適用された行動コード
- 環境のサマリ（全体採用率，価格，政策レベル）

分析指標
マクロパターン：
- 採用率曲線（時間 t に対する採用率）
- 50% / 80% 採用到達時間

行動の多様性・異質性：
- ターンごとの行動分布のエントロピー
- 属性別の採用率のばらつき

パス依存性：
- 初期ショック有無でのマクロパターン差分

現実データへの適合度（実データがある場合）：
- 採用率系列に対する RMSE, MAE
- 分布類似度（KLダイバージェンスなど）

LLM vs 従来ABMの比較指標：
- 同一条件でのマクロ指標の差分
- マイクロレベルでの選択の相関（どの程度同じ行動を選んでいるか）

仮説検証：
H1（マクロパターン再現＋多様性）：
- 従来ABMとLLMハイブリッドで，採用率曲線を重ねて比較し，RMSEが小さいかどうかを確認。
- 同時に，行動分布エントロピーや属性別ばらつきを比較し，LLMモデルの方が多様性が高いか検証。

H2（現実データへのフィット）：
- 実観測データがある場合，それぞれのモデルの採用率系列に対するRMSE/MAEを計算し，LLMモデルが有意に低いか統計的検定。

H3（異質性表現）：
- プロンプトで指定した性格・価値観ごとに行動傾向（協調的か，自利的かなど）を集計し，設計した異質性が実際の行動分布に反映されているかを確認。
"""

ABM_METRICS = [
    "adoption_rate_curve",           # 採用率曲線
    "time_to_50_percent_adoption",   # 50%採用到達時間
    "time_to_80_percent_adoption",   # 80%採用到達時間
    "behavior_entropy",              # 行動分布エントロピー
    "adoption_rate_variance_by_attr",# 属性別採用率ばらつき
    "path_dependency_diff",          # パス依存性（ショック有無での差分）
    "rmse_vs_real_data",             # 実データとのRMSE
    "mae_vs_real_data",              # 実データとのMAE
    "kl_divergence",                 # KLダイバージェンス
    "llm_vs_rule_macro_diff",        # LLM vs 従来ABMマクロ指標差分
    "micro_choice_correlation",      # マイクロレベル選択相関
]


# =============================================================================
# PGG-S (Public Goods Game with Sanctions) - 制裁制度付き公共財ゲーム
# =============================================================================
PGG_SANCTION_TASK_DESC = """
シミュレーション検討シート
テーマ：制裁制度付き公共財ゲームにおけるLLMエージェントの協力行動

1. シミュレーション要求

背景・文脈:
複数の LLM エージェントを自律的に動かす際、公共財ゲームのような「共有資源」に対して協力するか、フリーライドするかは重要な行動特性となる。とくに推論能力が高いモデルと従来型モデルで、協力・フリーライド・制裁行動の違いが指摘されており、それを再現・比較する必要がある。

目的:
- 制裁制度付き公共財ゲームにおける LLM エージェントの協力・フリーライド行動を定量評価する。
- 推論系モデルと従来型モデルの行動パターン（協力水準、制度選択、制裁の使い方）を比較する。
- 制裁コストがある状況で、各モデルがどのように規範維持（norm enforcement）を行うかを把握する。

研究質問:
RQ1: LLM エージェントは制度選択付き公共財ゲームでどの程度協力を維持できるか。
RQ2: 推論系モデルと従来型モデルでは、協力率・フリーライダー率・制裁の方向性がどう異なるか。
RQ3: 制裁制度（SI）と非制裁制度（SFI）の選好パターンはモデルごとにどう異なるか。

仮説:
H1: 推論系 LLM は従来型より協力率が低く、フリーライドに傾きやすい。
H2: 従来型 LLM は SI を選好し、制裁を積極的に用いて協力維持を試みる。
H3: 制裁コストに敏感なモデルほど制裁を避け、結果として協力水準が崩れやすい。

その他（任意）:
人間の経済実験で再現された現象との比較を意識し、エンドウメントや乗数などのパラメータは実験研究に近い範囲を採用する。

2. シミュレーション要件

エージェント
人数:
- 基本設定: 7 エージェント
- 拡張: 4〜10 の範囲で感度分析可能

ロールと説明:
全プレイヤーは同質の LLM エージェント
ラウンドごとに以下を決定する（対称プレイヤー）
- 制度選択（SI / SFI）
- 公共財への拠出額
- SI 所属者は他プレイヤーへの報酬／罰の付与

エージェントの状態（記憶・内部状態・行動）:
内部状態
- 直近 5 ラウンド分の自身の行動・利得
- 匿名化された全プレイヤーの行動履歴
- 所持トークン、現在ラウンド番号

行動空間
- institution_choice ∈ {SI, SFI}
- contribution ci ∈ [0, e]
- sanction 行動（報酬/罰の配分）

エージェントの状態更新:
- 各ラウンド終了時に利得を計算し所持トークン更新
- 行動履歴を最新ラウンド情報で更新
- 履歴は最大 5 ラウンド分まで保持

エージェントと環境との相互作用:
環境 → エージェント
- ゲームルール、直近履歴、制度の説明、利得計算方法
- 「どの制度を選ぶか」「いくつ拠出するか」「誰に制裁を与えるか」などのタスク提示

エージェント → 環境
- 自然言語推論 + 数値形式での最終回答
- 環境が数値化・利得計算し次ラウンドへ反映

環境
環境の構造:
- ラウンド制
- プレイヤー数、乗数、制裁強度などのパラメータ
- 各制度（SI/SFI）ごとの公共財プール

環境の状態仕様:
state_t = {t, params, agent_states, institution_pools, history}
agent_states は各エージェントの制度選択・拠出・制裁・利得を含む

環境の更新ルール:
1. エージェントの制度選択・拠出を取得
2. SI/SFI ごとに公共財プールを集計
3. 基礎利得を計算
4. SI に限り制裁ステージで利得調整
5. 最終利得を反映し履歴に記録
6. 次ラウンドへ進める

プロトコル
ターン/ラウンド/タイムステップ構造:
1 ラウンド = 制度選択 → 拠出 → 制裁 → 結果提示

最大反復数:
15 ラウンド（基本設定）

終了条件:
- ラウンド到達で終了
- 任意で「協力崩壊」などの早期終了ルールも追加可能

フェーズ構造（任意）:
- Phase 1: ベースライン
- Phase 2: パラメータ感度分析
- Phase 3: プロンプト条件の比較

対話フロー:
- 各ステージで環境が個別エージェントに問い合わせ、応答を数値化して処理
- エージェント同士の直接会話はなし

ルール
共有情報:
- 制度ルール、乗数、制裁仕様、各ラウンドの匿名履歴
- SI/SFI の特徴と利得計算方法

非公開情報:
- エージェントの内部推論テキスト
- エージェント ID とモデル種類の対応（実験者のみ保持）

意思決定ルール（任意）:
- 明示的な計算式ではなく、プロンプトに基づく LLM の自然言語推論に任せる
- 最後に Answer: の形式で数値を返させる

利得構造（任意）:
基礎利得
πi = (e − ci) + α * (平均貢献 × 人数補正)

制裁利得
報酬/罰の効果とコストを反映

最終利得
π_total_i = 上記の合算

実験条件:
- モデル種類
- 推論設定（reasoning depth）
- 温度・top_p
- 制裁強度
- 初期の制度分布

ログ
ログ形式:
1 行 = run × round × agent

記録すべき内容:
- run_id, round, agent_id, model_type
- institution_choice
- contribution
- sanctions_given / sanctions_received
- payoff_base / payoff_sanction / payoff_total
- 行動分類（高拠出／低拠出）
- プロンプト・推論テキスト・最終回答

分析指標
協力関連:
- 平均拠出額
- 高拠出者比率
- フリーライダー比率

制度関連:
- SI 選択率
- punish/reward 比率

成果関連:
- 平均利得
- 制裁コスト総額

行動パターン:
- 協力維持／崩壊／固定戦略の分類

仮説検証:
H1: モデル種類ごとに協力率・拠出分布を比較
H2: SI 選択率と制裁行動の違いを比較
H3: 制裁コストと協力崩壊の相関を分析
"""

PGG_SANCTION_METRICS = [
    # 協力関連
    "average_contribution",          # 平均拠出額
    "high_contributor_ratio",        # 高拠出者比率
    "free_rider_ratio",              # フリーライダー比率
    "cooperation_rate",              # 協力率
    # 制度関連
    "si_selection_rate",             # SI（制裁制度）選択率
    "sfi_selection_rate",            # SFI（非制裁制度）選択率
    "punishment_ratio",              # 罰の使用比率
    "reward_ratio",                  # 報酬の使用比率
    "punish_reward_ratio",           # punish/reward 比率
    # 成果関連
    "average_payoff",                # 平均利得
    "total_sanction_cost",           # 制裁コスト総額
    "payoff_variance",               # 利得の分散
    # 行動パターン
    "cooperation_maintenance_rate",  # 協力維持率
    "cooperation_collapse_round",    # 協力崩壊ラウンド
    "strategy_fixation_rate",        # 固定戦略率
    # モデル比較
    "reasoning_vs_standard_coop_diff",  # 推論系vs従来型の協力率差
]


# =============================================================================
# 実験レジストリ
# =============================================================================
EXPERIMENTS = {
    "tpgg": {
        "name": "Threshold Public Goods Game",
        "task_desc": TPGG_TASK_DESC,
        "metrics": TPGG_METRICS,
    },
    "bbs": {
        "name": "LLM Bulletin Board Influence Model",
        "task_desc": BBS_TASK_DESC,
        "metrics": BBS_METRICS,
    },
    "abm": {
        "name": "LLM-driven ABM vs Rule-based ABM",
        "task_desc": ABM_TASK_DESC,
        "metrics": ABM_METRICS,
    },
    "pgg_sanction": {
        "name": "Public Goods Game with Sanctions",
        "task_desc": PGG_SANCTION_TASK_DESC,
        "metrics": PGG_SANCTION_METRICS,
    },
}

# デフォルト実験
DEFAULT_EXPERIMENT = "tpgg"


def get_experiment(name: str = None) -> dict:
    """
    実験設定を取得

    Args:
        name: 実験名 (None の場合はデフォルト)

    Returns:
        {"name": str, "task_desc": str, "metrics": list}
    """
    if name is None:
        name = DEFAULT_EXPERIMENT
    if name not in EXPERIMENTS:
        raise ValueError(f"Unknown experiment: {name}. Available: {list(EXPERIMENTS.keys())}")
    return EXPERIMENTS[name]


def list_experiments() -> list:
    """利用可能な実験一覧を取得"""
    return list(EXPERIMENTS.keys())
