"""
実験タスクとメトリクスの定義
新しい実験を追加する場合はここに追記してください
"""

# =============================================================================
# TPGG (Threshold Public Goods Game)
# =============================================================================
TPGG_TASK_DESC = """
シミュレーション検討シート（TPGG）

## 🎯 1. シミュレーション要求

### 背景・文脈
4人グループで、**一定額のトークンを共同で拠出できればご褒美（V）がもらえる**ゲームにおける、**グループに提示される拠出目安（ルール）の性質**が、実際の行動にどう影響するかをLLMエージェントを用いて調査する。ルールは「ご褒美獲得に必要な合計額（しきい値T）に**ちょうど**合わせるもの」と「必要額より**多めに**要求するもの」の2パターンで比較する。

### 目的
グループに示される「出してほしい金額の目安（ルール）」が、「**ちょうど必要な合計**」か「**必要以上に多い合計**」かによって、LLMエージェントの**行動**、**結果（達成率）**、**効率（過剰拠出）**がどう変わるのかを調べる。

### 研究質問
* ルールが「必要以上に多い（多めの要求）」だと、**行動が乱れやすい**か？
* 必要な分ちょうどのルールは、むしろ**安定した協力を生む**か？
* グループ全体がちょうど必要額に合わせる「**効率の良さ**」はどう変わるか？

### 仮説
* ① **多めに要求されたルール**は、守る人が減りやすい。（協調性の低下）
* ② 必要額を達成できるかどうかは、**ルールの違いではあまり変わらない**。（目標達成率への影響は小さい）
* ③ **多めのルール**は「出しすぎ（無駄）」を増やし、**効率を下げる**。（過剰拠出の増加）
* ④ 1人あたりの負担が**ピッタリ均等割りできる場合**、協力がまとまりやすい。（FAIR条件の優位性）

***

## ⚙️ 2. シミュレーション要件

### エージェント
* **人数:** 4人（1グループ）
* **ロールと説明:** 4人とも同じ立場。各ラウンドで、自分の持ちトークン10のうち、何トークンを共同の箱に入れるか選ぶ。全員の合計がしきい値 (T) を超えれば、ご褒美 (V) がもらえる。
* **記憶・内部状態:** 過去の自分の拠出額、グループ合計の拠出額、自分の得点、**（後半：ラウンド11〜20のみ）**自分に示された「出してほしい額（ルール）」。
* **更新ルール:** ラウンドの最後に、そのラウンドの情報を記録し、次のラウンドの意思決定の参考にする。

### プロトコル
* **ターン/ラウンド構造:** 1回のゲームは**20ラウンド**。各ラウンドは、①4人が同時に出す額を決める → ②合計額を見る → ③しきい値を超えたかどうか判定 → ④得点を返す、の順で進行する。
* **最大ラウンド数:** 20
* **終了条件:** 20ラウンド終了。
* **各工程の試行数:** 1つの設定につき、4人グループを**複数回（例：11グループ）**まわす。
* **フェーズ構造:**
    * **ラウンド1〜10:** ルールなし（ベースライン）
    * **ラウンド11〜20:** 設定に応じたルールを提示（またはルールなし）

### 環境・ルール
* **ネットワーク構造:** 4人は同じグループ。グループ間の交流はなし（閉じた小世界で毎回意思決定）。
* **行動空間 / アクションセット:** 0〜10の整数から1つ選んで拠出する。
* **共有情報:** 各自の持ちトークン10、しきい値 (T)（条件ごとに違う）、**（後半）**みんなの「出してほしい額（ルール）」、ラウンド後の**合計拠出額**、**自分の得点**。
* **非公開情報:** 他のメンバーが実際にいくら出したか（自分の分は見えている）、各メンバーの考え・意図。

### 必要なルール、利得構造
利得（1人あたり）：自分が出した額を $c_i$、全員の合計を $C = \sum c_i$ とする。
* **しきい値未達成** ($C < T$): $$\pi_i = 10 - c_i$$
* **しきい値達成** ($C \ge T$): $$\pi_i = 10 - c_i + V$$
* **補足:** しきい値を超えた分には追加のご褒美なし（＝出しすぎは**「無駄」**）。ご褒美額 $V$ は、例えば $V=10$ などとする（高すぎず低すぎない値）。

### 実験条件（＝比較する設定の一覧）
5つの設定（$V$ は全条件で一定とする）：

| 設定名 | T（しきい値） | ルール (R1, R2, R3, R4) | 合計ルール額 $\sum R_i$ | 特徴 |
| :--- | :--- | :--- | :--- | :--- |
| **FAIRSUFF** | 20 | (5, 5, 5, 5) | 20 | **必要額ちょうど**かつ**均等割りOK** |
| **FAIRINF** | 20 | (5, 5, 6, 6) | 22 | **多めの要求**かつ**均等割りOK**（ただしルールは不均等） |
| **UNFAIRSUFF** | 22 | (5, 5, 6, 6) | 22 | **必要額ちょうど**かつ**均等割り不可** |
| **UNFAIRINF** | 22 | (6, 6, 6, 6) | 24 | **多めの要求**かつ**均等割り不可** |
| **CONTROL** | 22 | ルールなし | - | **ルールなし**（ベースライン） |

***

## 📊 3. ログ・分析指標

### ログ形式
CSV または JSONL（1行が1ラウンド）

### 記録すべき内容
* **設定情報:** どの設定（FAIRSUFF, FAIRINF, etc.）で行ったか
* **ラウンド情報:** ラウンド番号
* **行動:** 各メンバーの出した額 ($c_1, c_2, c_3, c_4$)、合計出した額 ($C$)
* **結果:** しきい値達成の有無（True/False）
* **利得:** 各メンバーの得点 ($\pi_1, \pi_2, \pi_3, \pi_4$)
* **ルールとの関係（R11-20のみ）:** 各メンバーの**ルールを守ったかのフラグ**（$c_i = R_i$ ならTrue、そうでないならFalse）

### 分析指標
* **しきい値達成率:** 20ラウンド中、しきい値 ($T$) を達成したラウンドの割合（成功の割合）。
* **平均の出した額:** グループ全体の1ラウンドあたりの**平均拠出額**（$\text{Average } C$）。
* **必要額よりどれだけ多く出たか（過剰分）:** $\text{Average } (C - T)$。しきい値達成ラウンドのみ、または全ラウンドで計算。これが**「無駄」**の指標となる。
* **ルールを守った割合（R11-20のみ）:** 各メンバーが、提示されたルール額 ($R_i$) と**同じ額を拠出した割合**。
* **10ラウンド目→11ラウンド目の変化:** ルール導入前後（R10とR11以降）での上記指標の**変化率**（ルール導入効果）。
"""

TPGG_METRICS = [
    "threshold_achievement_rate",
    "average_contribution",
    "excess_contribution",
    "rule_compliance_rate",
]




# =============================================================================
# ABM (Agent-Based Model) - LLM駆動ABM vs 従来ルールベースABM比較
# =============================================================================
ABM_TASK_DESC = """
シミュレーション検討シート
テーマ：LLM駆動ABM vs 従来ルールベースABM比較

1. シミュレーション要求
背景・文脈：
従来のABMでは，エージェントの意思決定は「しきい値ルール」「単純な効用関数」「強化学習方策」など，事前に固定されたルールで与えられていることが多い。その結果，
- 人間らしい多様な意思決定・説明可能な行動が表現しにくい
- テキスト情報や複雑な文脈を直接扱えない
といった制約がある。
近年，LLMをエージェントの意思決定モジュールとして組み込むことで，ABMに人間らしい認知・推論・コミュニケーションを導入する試みが増えている。

目的：
従来のルールベースABMと，LLM駆動ABMの間で，
- マクロなパターン（交通流・感染拡大・技術採用曲線など）
- 行動の多様性・パス依存性
を比較すること。

LLMを意思決定に用いることで，どの条件で現実データ（観測された交通・感染・技術採用パターンなど）への適合度が向上するかを評価すること。

研究質問：
RQ1: LLMベースの意思決定に置き換えたABMは，従来ルールベースABMと比べて，交通・感染・技術採用などの既知のマクロパターンをどの程度再現できるか。
RQ2: どのような環境条件・プロンプト設計・LLM設定（温度，メモリ長など）のとき，従来ABMよりも現実データへのフィットが改善するか。
RQ3: LLMを用いることで，エージェントの異質性（価値観・リスク態度・社会規範など）はどの程度自然に表現・制御できるか。

仮説：
H1: 同じマクロ条件（価格，政策，ネットワーク構造など）の下で，LLM駆動ABMは従来ABMと同様のマクロパターン（例：S字型の技術採用曲線）を再現しつつ，個票レベルではより多様で説明可能な行動を示す。
H2: テキスト情報（ニュース，広告文，政策説明文など）が重要なドメインでは，LLM駆動ABMの方が従来ABMよりも実データへの当てはまり（RMSE等）が良い。
H3: プロンプト内でエージェントの属性・性格を明示することで，異質性をパラメトリックに制御しつつ，従来より少ないパラメータ設定で複雑な行動分布を得られる。

その他（任意）：
初期段階では「技術採用ABM（例：EV普及）」をベースラインとし，将来的に交通・感染モデルへ拡張する統一フレームワークを想定。
実装は後にAutoGen/AG2等のフレームワーク上で自動実験可能な形に整理することを前提。

2. シミュレーション要件
エージェント
人数：
個人エージェント 100人。
感度分析として人数を増減させる。

ロールと説明：
一般消費者エージェント：
- 新技術（例：EV）を採用するかどうかを意思決定。
- 所得，リスク許容度，社会的影響への感受性，環境意識などの属性を持つ。

（拡張）政策決定者エージェント：
- 補助金や規制の水準をラウンドごとに設定（ここもLLM化可能だが，初期バージョンでは固定ルールでもよい）。

エージェントの状態（記憶・内部状態（メモリー）、行動などの仕様）：
静的属性：所得層，年齢層，リスク嗜好（リスク回避〜リスク志向），社会的タイプ（同調的・自律的など）。

動的状態：
- 技術採用状態（未採用／採用済み／乗り換え検討中など）
- 近隣・友人の採用状況の履歴
- 直近数ターンの支出・満足度

メモリ：
- 過去数ターンの観測・自分の選択・結果を要約したテキスト（LLMへのコンテキスト）。

エージェントの状態更新：
各ターンの初めに，環境から観測（価格，補助金，周囲の採用率など）を取得。

シミュレーションエンジンが，
- エージェントの属性・直近メモリ
- 観測された環境状態
をまとめてプロンプトを構築し，LLMに「今ターン，そのエージェントが取るべき行動（採用/未採用など）」を問い合わせる。

LLMの出力（テキスト）をパースして，行動カテゴリにマッピング。
行動に基づいてエージェントの状態（採用状態，メモリ）を更新。

エージェントと環境との相互作用：
エージェントの採用行動が，
- 市場の採用率
- 技術の普及段階
- （拡張）交通需要や感染率
に影響し，次ターンの環境状態として他エージェントに返る。

環境
環境の構造：
社会ネットワーク（グラフ）：
- ノード＝個人エージェント，エッジ＝社会的つながり。
- Small-world / scale-free グラフなどを選択。

市場レベルの集計状態：
- 全体採用率，価格，補助金水準，メディア露出度など。

環境の状態仕様：
グローバル変数：
- 時刻 t
- 採用率，価格，政策レベル
- （オプション）外生ショック（燃料価格高騰など）

ローカル変数（エージェント視点で提示）：
- 近隣の採用率
- 所属コミュニティの平均属性など。

環境の更新ルール：
各ターン後，
- 全エージェントの行動から採用率を再計算。
- 採用率に応じて価格・補助金を調整するシナリオを定義（政策フィードバック）。

交通・感染モデルに拡張する場合は，
- 移動ルート選択 → 混雑度更新
- 接触パターン → 感染率更新
などのドメイン固有の更新式を持つ。

プロトコル
ターン/ラウンド/タイムステップ構造：
離散時間ステップ t = 1, 2, …, T
各ステップで「観測 → LLM呼び出し → 行動 → 環境更新」の順に処理。

最大反復数：
デフォルト T = 100 ターン（技術採用が飽和するまでの時間を想定）。

終了条件：
いずれかを満たした時点で終了：
- 採用率が 95% 以上で 10 ターン連続で変化が小さい
- あるいは t = T に到達。

各工程の試行数：
1条件あたり
- 「従来ルールABM」「LLMハイブリッドABM」を同じシードでペア実行。

フェーズ構造（任意）：
- フェーズ1：ベースライン（政策なし／初期価格一定）
- フェーズ2：政策介入（補助金導入，規制強化など）
- フェーズ3：ショック（燃料価格急騰など）

対話フロー：
シミュレーションエンジンが，各エージェントごとにプロンプトを生成：
- 「あなたは○○な属性を持つ消費者です…」
- 「現在の状況は…」
- 「今ターン，どの選択をしますか？ A: 採用する, B: まだ様子を見る など」

LLMがテキストで理由＋選択肢を返答。
エンジン側で選択肢をパースして行動コードに変換。
必要なら，エージェント同士の会話（口コミ）フェーズも追加可能。

ルール
共有情報（全員が知っている設定・公表情報）：
- 技術の基本的なメリット・デメリット（価格，維持費，環境負荷など）。
- 政策・補助金のルール。
- 行動の選択肢（採用する／しない／保留 など）。

非公開情報（エージェントごとに違う秘密情報）：
- 個人の真の所得，貯蓄制約。
- リスク嗜好，環境意識の強さ。
- 一部のローカル情報（友人から聞いた口コミなど）。

意思決定ルール（任意）：
従来ABM条件：
- ロジットモデルやしきい値モデルで「採用確率」を計算し，確率的に行動。

LLMハイブリッド条件：
- プロンプト内に「あなたの目的は長期的な満足度を最大化すること」「収入制約を超える支出は避けること」などを明示し，
- LLMに推論させて行動を選ばせる。
- 出力は {ADOPT, WAIT, NEVER} などの有限集合にマッピング。

利得構造（任意）：
1ターンごとの効用
u_t = 便益（運転コスト削減・満足度など） − 費用（購入コストの割引分）

長期効用
割引和 ∑_t γ^t u_t を最大化するような行動が望ましい，とLLMプロンプト内に記述。

実験条件：
- ベースライン：従来ルールABMのみ。
- ハイブリッド：エージェントの意思決定をすべてLLMに置き換え。
- ミックス：一部の属性クラス（例：高所得層）のみLLM駆動，他は従来ルール。

LLM内部パラメータ：
- 温度（0.2 / 0.7）
- メモリ長（直近 3 ターン vs 10 ターン）
- プロンプトの詳細度（簡潔 vs 詳細）

ログ
記録すべき内容：
- 時刻 t, エージェントID i
- 状態前後（採用状態，属性の要約）
- LLMプロンプトIDとその要約（完全なテキストは容量の都合で別ストレージに保存）
- LLM応答（選択肢と理由テキスト）
- 実際に適用された行動コード
- 環境のサマリ（全体採用率，価格，政策レベル）

分析指標
マクロパターン：
- 採用率曲線（時間 t に対する採用率）
- 50% / 80% 採用到達時間

行動の多様性・異質性：
- ターンごとの行動分布のエントロピー
- 属性別の採用率のばらつき

パス依存性：
- 初期ショック有無でのマクロパターン差分

現実データへの適合度（実データがある場合）：
- 採用率系列に対する RMSE, MAE
- 分布類似度（KLダイバージェンスなど）

LLM vs 従来ABMの比較指標：
- 同一条件でのマクロ指標の差分
- マイクロレベルでの選択の相関（どの程度同じ行動を選んでいるか）

仮説検証：
H1（マクロパターン再現＋多様性）：
- 従来ABMとLLMハイブリッドで，採用率曲線を重ねて比較し，RMSEが小さいかどうかを確認。
- 同時に，行動分布エントロピーや属性別ばらつきを比較し，LLMモデルの方が多様性が高いか検証。

H2（現実データへのフィット）：
- 実観測データがある場合，それぞれのモデルの採用率系列に対するRMSE/MAEを計算し，LLMモデルが有意に低いか統計的検定。

H3（異質性表現）：
- プロンプトで指定した性格・価値観ごとに行動傾向（協調的か，自利的かなど）を集計し，設計した異質性が実際の行動分布に反映されているかを確認。
"""

ABM_METRICS = [
    "adoption_rate_curve",           # 採用率曲線
    "time_to_50_percent_adoption",   # 50%採用到達時間
    "time_to_80_percent_adoption",   # 80%採用到達時間
    "behavior_entropy",              # 行動分布エントロピー
    "adoption_rate_variance_by_attr",# 属性別採用率ばらつき
    "path_dependency_diff",          # パス依存性（ショック有無での差分）
    "rmse_vs_real_data",             # 実データとのRMSE
    "mae_vs_real_data",              # 実データとのMAE
    "kl_divergence",                 # KLダイバージェンス
    "llm_vs_rule_macro_diff",        # LLM vs 従来ABMマクロ指標差分
    "micro_choice_correlation",      # マイクロレベル選択相関
]


# =============================================================================
# PGG-S (Public Goods Game with Sanctions) - 制裁制度付き公共財ゲーム
# =============================================================================
PGG_SANCTION_TASK_DESC = """
シミュレーション検討シート
テーマ：制裁制度付き公共財ゲームにおけるLLMエージェントの協力行動

1. シミュレーション要求

背景・文脈:
複数の LLM エージェントを自律的に動かす際、公共財ゲームのような「共有資源」に対して協力するか、フリーライドするかは重要な行動特性となる。とくに推論能力が高いモデルと従来型モデルで、協力・フリーライド・制裁行動の違いが指摘されており、それを再現・比較する必要がある。

目的:
- 制裁制度付き公共財ゲームにおける LLM エージェントの協力・フリーライド行動を定量評価する。
- 推論系モデルと従来型モデルの行動パターン（協力水準、制度選択、制裁の使い方）を比較する。
- 制裁コストがある状況で、各モデルがどのように規範維持（norm enforcement）を行うかを把握する。

研究質問:
RQ1: LLM エージェントは制度選択付き公共財ゲームでどの程度協力を維持できるか。
RQ2: 推論系モデルと従来型モデルでは、協力率・フリーライダー率・制裁の方向性がどう異なるか。
RQ3: 制裁制度（SI）と非制裁制度（SFI）の選好パターンはモデルごとにどう異なるか。

仮説:
H1: 推論系 LLM は従来型より協力率が低く、フリーライドに傾きやすい。
H2: 従来型 LLM は SI を選好し、制裁を積極的に用いて協力維持を試みる。
H3: 制裁コストに敏感なモデルほど制裁を避け、結果として協力水準が崩れやすい。

その他（任意）:
人間の経済実験で再現された現象との比較を意識し、エンドウメントや乗数などのパラメータは実験研究に近い範囲を採用する。

2. シミュレーション要件

エージェント
人数:
- 基本設定: 7 エージェント
- 拡張: 4〜10 の範囲で感度分析可能

ロールと説明:
全プレイヤーは同質の LLM エージェント
ラウンドごとに以下を決定する（対称プレイヤー）
- 制度選択（SI / SFI）
- 公共財への拠出額
- SI 所属者は他プレイヤーへの報酬／罰の付与

エージェントの状態（記憶・内部状態・行動）:
内部状態
- 直近 5 ラウンド分の自身の行動・利得
- 匿名化された全プレイヤーの行動履歴
- 所持トークン、現在ラウンド番号

行動空間
- institution_choice ∈ {SI, SFI}
- contribution ci ∈ [0, e]
- sanction 行動（報酬/罰の配分）

エージェントの状態更新:
- 各ラウンド終了時に利得を計算し所持トークン更新
- 行動履歴を最新ラウンド情報で更新
- 履歴は最大 5 ラウンド分まで保持

エージェントと環境との相互作用:
環境 → エージェント
- ゲームルール、直近履歴、制度の説明、利得計算方法
- 「どの制度を選ぶか」「いくつ拠出するか」「誰に制裁を与えるか」などのタスク提示

エージェント → 環境
- 自然言語推論 + 数値形式での最終回答
- 環境が数値化・利得計算し次ラウンドへ反映

環境
環境の構造:
- ラウンド制
- プレイヤー数、乗数、制裁強度などのパラメータ
- 各制度（SI/SFI）ごとの公共財プール

環境の状態仕様:
state_t = {t, params, agent_states, institution_pools, history}
agent_states は各エージェントの制度選択・拠出・制裁・利得を含む

環境の更新ルール:
1. エージェントの制度選択・拠出を取得
2. SI/SFI ごとに公共財プールを集計
3. 基礎利得を計算
4. SI に限り制裁ステージで利得調整
5. 最終利得を反映し履歴に記録
6. 次ラウンドへ進める

プロトコル
ターン/ラウンド/タイムステップ構造:
1 ラウンド = 制度選択 → 拠出 → 制裁 → 結果提示

最大反復数:
15 ラウンド（基本設定）

終了条件:
- ラウンド到達で終了
- 任意で「協力崩壊」などの早期終了ルールも追加可能

フェーズ構造（任意）:
- Phase 1: ベースライン
- Phase 2: パラメータ感度分析
- Phase 3: プロンプト条件の比較

対話フロー:
- 各ステージで環境が個別エージェントに問い合わせ、応答を数値化して処理
- エージェント同士の直接会話はなし

ルール
共有情報:
- 制度ルール、乗数、制裁仕様、各ラウンドの匿名履歴
- SI/SFI の特徴と利得計算方法

非公開情報:
- エージェントの内部推論テキスト
- エージェント ID とモデル種類の対応（実験者のみ保持）

意思決定ルール（任意）:
- 明示的な計算式ではなく、プロンプトに基づく LLM の自然言語推論に任せる
- 最後に Answer: の形式で数値を返させる

利得構造（任意）:
基礎利得
πi = (e − ci) + α * (平均貢献 × 人数補正)

制裁利得
報酬/罰の効果とコストを反映

最終利得
π_total_i = 上記の合算

実験条件:
- モデル種類
- 推論設定（reasoning depth）
- 温度・top_p
- 制裁強度
- 初期の制度分布

ログ
ログ形式:
1 行 = run × round × agent

記録すべき内容:
- run_id, round, agent_id, model_type
- institution_choice
- contribution
- sanctions_given / sanctions_received
- payoff_base / payoff_sanction / payoff_total
- 行動分類（高拠出／低拠出）
- プロンプト・推論テキスト・最終回答

分析指標
協力関連:
- 平均拠出額
- 高拠出者比率
- フリーライダー比率

制度関連:
- SI 選択率
- punish/reward 比率

成果関連:
- 平均利得
- 制裁コスト総額

行動パターン:
- 協力維持／崩壊／固定戦略の分類

仮説検証:
H1: モデル種類ごとに協力率・拠出分布を比較
H2: SI 選択率と制裁行動の違いを比較
H3: 制裁コストと協力崩壊の相関を分析
"""

PGG_SANCTION_METRICS = [
    # 協力関連
    "average_contribution",          # 平均拠出額
    "high_contributor_ratio",        # 高拠出者比率
    "free_rider_ratio",              # フリーライダー比率
    "cooperation_rate",              # 協力率
    # 制度関連
    "si_selection_rate",             # SI（制裁制度）選択率
    "sfi_selection_rate",            # SFI（非制裁制度）選択率
    "punishment_ratio",              # 罰の使用比率
    "reward_ratio",                  # 報酬の使用比率
    "punish_reward_ratio",           # punish/reward 比率
    # 成果関連
    "average_payoff",                # 平均利得
    "total_sanction_cost",           # 制裁コスト総額
    "payoff_variance",               # 利得の分散
    # 行動パターン
    "cooperation_maintenance_rate",  # 協力維持率
    "cooperation_collapse_round",    # 協力崩壊ラウンド
    "strategy_fixation_rate",        # 固定戦略率
    # モデル比較
    "reasoning_vs_standard_coop_diff",  # 推論系vs従来型の協力率差
]



# =============================================================================
# 実験レジストリ
# =============================================================================
EXPERIMENTS = {
    "tpgg": {
        "name": "Threshold Public Goods Game",
        "task_desc": TPGG_TASK_DESC,
        "metrics": TPGG_METRICS,
    },
    "abm": {
        "name": "LLM-driven ABM vs Rule-based ABM",
        "task_desc": ABM_TASK_DESC,
        "metrics": ABM_METRICS,
    },
    "pgg_sanction": {
        "name": "Public Goods Game with Sanctions",
        "task_desc": PGG_SANCTION_TASK_DESC,
        "metrics": PGG_SANCTION_METRICS,
    },
}

# デフォルト実験
DEFAULT_EXPERIMENT = "tpgg"


def get_experiment(name: str = None) -> dict:
    """
    実験設定を取得

    Args:
        name: 実験名 (None の場合はデフォルト)

    Returns:
        {"name": str, "task_desc": str, "metrics": list}
    """
    if name is None:
        name = DEFAULT_EXPERIMENT
    if name not in EXPERIMENTS:
        raise ValueError(f"Unknown experiment: {name}. Available: {list(EXPERIMENTS.keys())}")
    return EXPERIMENTS[name]


def list_experiments() -> list:
    """利用可能な実験一覧を取得"""
    return list(EXPERIMENTS.keys())
